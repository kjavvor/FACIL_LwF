PLATT 5 poch | 4 tasks

============================================================================================================
Arguments =
        approach: finetuning
        batch_size: 64
        clipping: 10000
        datasets: ['cifar100']
        eval_on_train: False
        exp_name: None
        fix_bn: False
        gpu: 0
        gridsearch_tasks: -1
        keep_existing_head: False
        last_layer_analysis: False
        log: ['disk']
        lr: 0.1
        lr_factor: 3
        lr_min: 0.0001
        lr_patience: 5
        momentum: 0.0
        multi_softmax: False
        nc_first_task: None
        nepochs: 5
        network: resnet32
        no_cudnn_deterministic: False
        num_tasks: 4
        num_workers: 4
        pin_memory: False
        pretrained: False
        results_path: ../results
        save_models: False
        seed: 0
        stop_at_task: 0
        use_valid_only: False
        warmup_lr_factor: 1.0
        warmup_nepochs: 0
        weight_decay: 0.0
============================================================================================================
WARNING: [CUDA unavailable] Using CPU instead!
Approach arguments =
        all_outputs: False
============================================================================================================
Exemplars dataset arguments =
        exemplar_selection: random
        num_exemplars: 0
        num_exemplars_per_class: 0
============================================================================================================
WARNING: ../results\cifar100_finetuning already exists!
Files already downloaded and verified
Files already downloaded and verified
[(0, 25), (1, 25), (2, 25), (3, 25)]
************************************************************************************************************
Task  0
| Epoch   1, time= 99.2s | Train: skip eval | Valid: time= 21.3s loss=2.338, TAw acc= 31.0% | *
| Epoch   2, time= 96.0s | Train: skip eval | Valid: time= 17.8s loss=2.298, TAw acc= 30.5% | *
| Epoch   3, time=102.5s | Train: skip eval | Valid: time= 20.5s loss=2.115, TAw acc= 36.9% | *
| Epoch   4, time=114.0s | Train: skip eval | Valid: time= 21.4s loss=2.032, TAw acc= 37.4% | *
| Epoch   5, time= 99.6s | Train: skip eval | Valid: time= 21.9s loss=2.109, TAw acc= 39.9% |
------------------------------------------------------------------------------------------------------------
| Epoch   1, time= 99.2s | Train: skip eval | Valid: time= 21.3s loss=2.338, TAw acc= 31.0% | *
| Epoch   2, time= 96.0s | Train: skip eval | Valid: time= 17.8s loss=2.298, TAw acc= 30.5% | *
| Epoch   3, time=102.5s | Train: skip eval | Valid: time= 20.5s loss=2.115, TAw acc= 36.9% | *
| Epoch   4, time=114.0s | Train: skip eval | Valid: time= 21.4s loss=2.032, TAw acc= 37.4% | *
| Epoch   5, time= 99.6s | Train: skip eval | Valid: time= 21.9s loss=2.109, TAw acc= 39.9% |
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=2.595 | TAw acc= 21.9%, forg= 13.6%| TAg acc=  6.3%, forg= 29.2% <<<
>>> Test on task  1 : loss=2.005 | TAw acc= 38.2%, forg=  0.0%| TAg acc= 35.0%, forg=  0.0% <<<
Save at ../results\cifar100_finetuning
************************************************************************************************************
Task  2
************************************************************************************************************
| Epoch   1, time=102.3s | Train: skip eval | Valid: time= 22.0s loss=2.422, TAw acc= 29.6% | *
| Epoch   2, time=112.8s | Train: skip eval | Valid: time= 21.6s loss=2.195, TAw acc= 35.0% | *
| Epoch   3, time=109.4s | Train: skip eval | Valid: time= 21.2s loss=2.328, TAw acc= 29.2% |
| Epoch   4, time=105.3s | Train: skip eval | Valid: time= 21.6s loss=3.054, TAw acc= 27.8% |
| Epoch   5, time=108.0s | Train: skip eval | Valid: time= 20.7s loss=2.144, TAw acc= 36.0% | *
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=2.661 | TAw acc= 20.2%, forg= 15.3%| TAg acc=  2.2%, forg= 33.4% <<<
>>> Test on task  1 : loss=2.417 | TAw acc= 25.7%, forg= 12.6%| TAg acc=  9.7%, forg= 25.3% <<<
>>> Test on task  2 : loss=2.146 | TAw acc= 35.6%, forg=  0.0%| TAg acc= 30.8%, forg=  0.0% <<<
Save at ../results\cifar100_finetuning
************************************************************************************************************
Task  3
************************************************************************************************************
| Epoch   1, time=105.5s | Train: skip eval | Valid: time= 18.6s loss=2.247, TAw acc= 36.2% | *
| Epoch   2, time= 98.5s | Train: skip eval | Valid: time= 18.8s loss=1.961, TAw acc= 41.9% | *
| Epoch   3, time= 98.7s | Train: skip eval | Valid: time= 18.4s loss=2.062, TAw acc= 42.2% |
| Epoch   4, time=107.9s | Train: skip eval | Valid: time= 19.8s loss=2.124, TAw acc= 41.9% |
| Epoch   5, time=104.7s | Train: skip eval | Valid: time= 24.8s loss=2.167, TAw acc= 42.4% |
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=2.821 | TAw acc= 18.0%, forg= 17.6%| TAg acc=  0.6%, forg= 35.0% <<<
>>> Test on task  1 : loss=2.659 | TAw acc= 21.0%, forg= 17.3%| TAg acc=  4.2%, forg= 30.8% <<<
>>> Test on task  2 : loss=2.633 | TAw acc= 25.4%, forg= 10.2%| TAg acc= 14.0%, forg= 16.8% <<<
>>> Test on task  3 : loss=1.938 | TAw acc= 42.1%, forg=  0.0%| TAg acc= 28.7%, forg=  0.0% <<<
Save at ../results\cifar100_finetuning
************************************************************************************************************
TAw Acc
         35.6%   0.0%   0.0%   0.0%     Avg.: 35.6%
         21.9%  38.2%   0.0%   0.0%     Avg.: 30.1%
         20.2%  25.7%  35.6%   0.0%     Avg.: 27.2%
         18.0%  21.0%  25.4%  42.1%     Avg.: 26.6%
************************************************************************************************************
TAg Acc
         35.6%   0.0%   0.0%   0.0%     Avg.: 35.6%
          6.3%  35.0%   0.0%   0.0%     Avg.: 20.7%
          2.2%   9.7%  30.8%   0.0%     Avg.: 14.2%
          0.6%   4.2%  14.0%  28.7%     Avg.: 11.8%
************************************************************************************************************
TAw Forg
          0.0%   0.0%   0.0%   0.0%
         13.6%   0.0%   0.0%   0.0%     Avg.: 13.6%
         15.3%  12.6%   0.0%   0.0%     Avg.: 13.9%
         17.6%  17.3%  10.2%   0.0%     Avg.: 15.0%
************************************************************************************************************
TAg Forg
          0.0%   0.0%   0.0%   0.0%
         29.2%   0.0%   0.0%   0.0%     Avg.: 29.2%
         33.4%  25.3%   0.0%   0.0%     Avg.: 29.3%
         35.0%  30.8%  16.8%   0.0%     Avg.: 27.5%
************************************************************************************************************
[Elapsed time = 0.8 h]
Done!


************************************************************************************************************
************************************************************************************************************
************************************************************************************************************
************************************************************************************************************
************************************************************************************************************************************************************************************************************************
************************************************************************************************************


Arguments =
        approach: finetuning
        batch_size: 64
        clipping: 10000
        datasets: ['cifar100']
        eval_on_train: False
        exp_name: None
        fix_bn: False
        gpu: 0
        gridsearch_tasks: -1
        keep_existing_head: False
        last_layer_analysis: False
        log: ['disk']
        lr: 0.1
        lr_factor: 3
        lr_min: 0.0001
        lr_patience: 5
        momentum: 0.0
        multi_softmax: False
        nc_first_task: None
        nepochs: 2
        network: resnet32
        no_cudnn_deterministic: False
        num_tasks: 4
        num_workers: 4
        pin_memory: False
        pretrained: False
        results_path: ../results
        save_models: False
        seed: 0
        stop_at_task: 0
        use_valid_only: False
        warmup_lr_factor: 1.0
        warmup_nepochs: 0
        weight_decay: 0.0
============================================================================================================
WARNING: [CUDA unavailable] Using CPU instead!
Approach arguments =
        all_outputs: False
============================================================================================================
Exemplars dataset arguments =
        exemplar_selection: random
        num_exemplars: 0
        num_exemplars_per_class: 0
============================================================================================================
WARNING: ../results\cifar100_finetuning already exists!
Files already downloaded and verified
Files already downloaded and verified
[(0, 25), (1, 25), (2, 25), (3, 25)]
************************************************************************************************************
Task  0
************************************************************************************************************
| Epoch   1, time=137.1s | Train: skip eval | Valid: time= 31.7s loss=2.582, TAw acc= 21.4% | *
| Epoch   2, time=165.1s | Train: skip eval | Valid: time= 35.9s loss=2.575, TAw acc= 22.8% | *
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=2.599 | TAw acc= 21.3%, forg=  0.0%| TAg acc= 21.3%, forg=  0.0% <<<
Save at ../results\cifar100_finetuning
************************************************************************************************************
Task  1
************************************************************************************************************
| Epoch   1, time=129.5s | Train: skip eval | Valid: time= 41.0s loss=2.507, TAw acc= 23.5% | *
| Epoch   2, time=227.8s | Train: skip eval | Valid: time= 55.9s loss=2.397, TAw acc= 27.8% | *
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=2.851 | TAw acc= 17.0%, forg=  4.3%| TAg acc=  3.3%, forg= 18.0% <<<
>>> Test on task  1 : loss=2.411 | TAw acc= 28.3%, forg=  0.0%| TAg acc= 26.8%, forg=  0.0% <<<
Save at ../results\cifar100_finetuning
************************************************************************************************************
Task  2
************************************************************************************************************
| Epoch   1, time=238.3s | Train: skip eval | Valid: time= 49.2s loss=2.546, TAw acc= 23.5% | *
| Epoch   2, time=233.4s | Train: skip eval | Valid: time= 60.9s loss=2.390, TAw acc= 30.3% | *
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=2.888 | TAw acc= 14.7%, forg=  6.6%| TAg acc=  0.8%, forg= 20.6% <<<
>>> Test on task  1 : loss=2.684 | TAw acc= 19.3%, forg=  9.0%| TAg acc=  7.7%, forg= 19.1% <<<
>>> Test on task  2 : loss=2.483 | TAw acc= 27.2%, forg=  0.0%| TAg acc= 21.8%, forg=  0.0% <<<
Save at ../results\cifar100_finetuning
************************************************************************************************************
Task  3
************************************************************************************************************
| Epoch   1, time=235.4s | Train: skip eval | Valid: time= 39.5s loss=2.294, TAw acc= 32.2% | *
| Epoch   2, time=116.8s | Train: skip eval | Valid: time= 26.8s loss=2.379, TAw acc= 30.4% |
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=2.973 | TAw acc= 13.3%, forg=  8.0%| TAg acc=  1.2%, forg= 20.1% <<<
>>> Test on task  1 : loss=2.742 | TAw acc= 20.6%, forg=  7.7%| TAg acc=  5.9%, forg= 20.9% <<<
>>> Test on task  2 : loss=2.600 | TAw acc= 22.9%, forg=  4.3%| TAg acc=  9.1%, forg= 12.8% <<<
>>> Test on task  3 : loss=2.332 | TAw acc= 31.6%, forg=  0.0%| TAg acc= 20.0%, forg=  0.0% <<<
Save at ../results\cifar100_finetuning
************************************************************************************************************
TAw Acc
         21.3%   0.0%   0.0%   0.0%     Avg.: 21.3% 
         17.0%  28.3%   0.0%   0.0%     Avg.: 22.6%
         14.7%  19.3%  27.2%   0.0%     Avg.: 20.4%
         13.3%  20.6%  22.9%  31.6%     Avg.: 22.1%
************************************************************************************************************
TAg Acc
         21.3%   0.0%   0.0%   0.0%     Avg.: 21.3%
          3.3%  26.8%   0.0%   0.0%     Avg.: 15.0%
          0.8%   7.7%  21.8%   0.0%     Avg.: 10.1%
          1.2%   5.9%   9.1%  20.0%     Avg.:  9.1%
************************************************************************************************************
TAw Forg
          0.0%   0.0%   0.0%   0.0%
          4.3%   0.0%   0.0%   0.0%     Avg.:  4.3%
          6.6%   9.0%   0.0%   0.0%     Avg.:  7.8%
          8.0%   7.7%   4.3%   0.0%     Avg.:  6.7%
************************************************************************************************************
TAg Forg
          0.0%   0.0%   0.0%   0.0%
         18.0%   0.0%   0.0%   0.0%     Avg.: 18.0% 
         20.6%  19.1%   0.0%   0.0%     Avg.: 19.8%
         20.1%  20.9%  12.8%   0.0%     Avg.: 17.9%
************************************************************************************************************
[Elapsed time = 0.6 h]
Done!


LWF -------------------------------------------------------------------

============================================================================================================
Arguments =
        approach: lwf
        batch_size: 64
        clipping: 10000
        datasets: ['cifar100']
        eval_on_train: False
        exp_name: None
        fix_bn: False
        gpu: 0
        gridsearch_tasks: -1
        keep_existing_head: False
        last_layer_analysis: False
        log: ['disk']
        lr: 0.1
        lr_factor: 3
        lr_min: 0.0001
        lr_patience: 5
        momentum: 0.0
        multi_softmax: False
        nc_first_task: None
        nepochs: 2
        network: resnet32
        no_cudnn_deterministic: False
        num_tasks: 4
        num_workers: 4
        pin_memory: False
        pretrained: False
        results_path: ../results
        save_models: False
        seed: 0
        stop_at_task: 0
        use_valid_only: False
        warmup_lr_factor: 1.0
        warmup_nepochs: 0
        weight_decay: 0.0
============================================================================================================
WARNING: [CUDA unavailable] Using CPU instead!
Approach arguments =
        T: 2
        lamb: 1
============================================================================================================
Exemplars dataset arguments =
        exemplar_selection: random
        num_exemplars: 0
        num_exemplars_per_class: 0
============================================================================================================
WARNING: ../results\cifar100_lwf already exists!
Files already downloaded and verified
Files already downloaded and verified
[(0, 25), (1, 25), (2, 25), (3, 25)]
************************************************************************************************************
Task  0
************************************************************************************************************
| Epoch   1, time=123.2s | Train: skip eval | Valid: time= 33.9s loss=2.572, TAw acc= 22.5% | *
| Epoch   2, time=134.6s | Train: skip eval | Valid: time= 26.3s loss=2.493, TAw acc= 23.4% | *
Rozmiar logits: torch.Size([11250, 25])
Rozmiar labels: torch.Size([11250])
Original accuracy before calibration: 22.28%
C:\Users\kamil\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\linear_model\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):        
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression       
  n_iter_i = _check_optimize_result(
Wymiar calibrated_probs: (11250, 25)
C:\Users\kamil\OneDrive\Pulpit\PW\sem2\ZZSN\project\FACIL_LwF\src\approach\lwf.py:95: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  calibrated_acc = (torch.tensor(calibrated_probs).argmax(dim=1) == labels).float().mean().item()
Calibrated accuracy with platt method: 31.99%
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=2.505 | TAw acc= 23.1%, forg=  0.0%| TAg acc= 23.1%, forg=  0.0% <<<
Save at ../results\cifar100_lwf
************************************************************************************************************
Task  1
************************************************************************************************************
| Epoch   1, time=140.9s | Train: skip eval | Valid: time= 22.0s loss=5.522, TAw acc= 23.1% | *
| Epoch   2, time=120.1s | Train: skip eval | Valid: time= 21.1s loss=5.473, TAw acc= 27.4% | *
Rozmiar logits: torch.Size([11250, 50])
Rozmiar labels: torch.Size([11250])
Original accuracy before calibration: 18.12%
C:\Users\kamil\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\linear_model\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):        
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression       
  n_iter_i = _check_optimize_result(
Wymiar calibrated_probs: (11250, 25)
C:\Users\kamil\OneDrive\Pulpit\PW\sem2\ZZSN\project\FACIL_LwF\src\approach\lwf.py:95: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  calibrated_acc = (torch.tensor(calibrated_probs).argmax(dim=1) == labels).float().mean().item()
Calibrated accuracy with platt method: 0.00%
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=2.768 | TAw acc= 17.0%, forg=  6.1%| TAg acc=  6.9%, forg= 16.2% <<<
>>> Test on task  1 : loss=5.422 | TAw acc= 27.5%, forg=  0.0%| TAg acc= 22.0%, forg=  0.0% <<<
Save at ../results\cifar100_lwf
************************************************************************************************************
Task  2
************************************************************************************************************
| Epoch   1, time=145.4s | Train: skip eval | Valid: time= 32.6s loss=6.265, TAw acc= 23.4% | *
| Epoch   2, time=126.3s | Train: skip eval | Valid: time= 38.9s loss=6.080, TAw acc= 30.5% | *
Rozmiar logits: torch.Size([11250, 75])
Rozmiar labels: torch.Size([11250])
Original accuracy before calibration: 21.07%
C:\Users\kamil\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\linear_model\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):        
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression       
  n_iter_i = _check_optimize_result(
Wymiar calibrated_probs: (11250, 25)
C:\Users\kamil\OneDrive\Pulpit\PW\sem2\ZZSN\project\FACIL_LwF\src\approach\lwf.py:95: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  calibrated_acc = (torch.tensor(calibrated_probs).argmax(dim=1) == labels).float().mean().item()
Calibrated accuracy with platt method: 0.00%
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=2.838 | TAw acc= 16.5%, forg=  6.6%| TAg acc=  3.2%, forg= 19.9% <<<
>>> Test on task  1 : loss=5.539 | TAw acc= 23.6%, forg=  3.9%| TAg acc=  8.5%, forg= 13.5% <<<
>>> Test on task  2 : loss=6.121 | TAw acc= 28.5%, forg=  0.0%| TAg acc= 23.9%, forg=  0.0% <<<
Save at ../results\cifar100_lwf
************************************************************************************************************
Task  3
************************************************************************************************************
| Epoch   1, time=170.5s | Train: skip eval | Valid: time= 35.1s loss=6.539, TAw acc= 27.8% | *
| Epoch   2, time=180.9s | Train: skip eval | Valid: time= 48.5s loss=6.427, TAw acc= 33.5% | *
Rozmiar logits: torch.Size([11250, 100])
Rozmiar labels: torch.Size([11250])
Original accuracy before calibration: 24.87%
C:\Users\kamil\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\linear_model\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):        
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression       
  n_iter_i = _check_optimize_result(
Wymiar calibrated_probs: (11250, 25)
C:\Users\kamil\OneDrive\Pulpit\PW\sem2\ZZSN\project\FACIL_LwF\src\approach\lwf.py:95: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  calibrated_acc = (torch.tensor(calibrated_probs).argmax(dim=1) == labels).float().mean().item()
Calibrated accuracy with platt method: 0.00%
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=2.823 | TAw acc= 17.6%, forg=  5.5%| TAg acc=  1.1%, forg= 22.0% <<<
>>> Test on task  1 : loss=5.739 | TAw acc= 17.8%, forg=  9.7%| TAg acc=  1.4%, forg= 20.6% <<<
>>> Test on task  2 : loss=6.334 | TAw acc= 22.5%, forg=  6.0%| TAg acc=  7.8%, forg= 16.2% <<<
>>> Test on task  3 : loss=6.431 | TAw acc= 32.6%, forg=  0.0%| TAg acc= 28.0%, forg=  0.0% <<<
Save at ../results\cifar100_lwf
************************************************************************************************************
TAw Acc
         23.1%   0.0%   0.0%   0.0%     Avg.: 23.1%
********************
TAg Acc
         23.1%   0.0%   0.0%   0.0%     Avg.: 23.1%
          6.9%  22.0%   0.0%   0.0%     Avg.: 14.5%
          3.2%   8.5%  23.9%   0.0%     Avg.: 11.9%
          1.1%   1.4%   7.8%  28.0%     Avg.:  9.6%
************************************************************************************************************
TAw Forg
          0.0%   0.0%   0.0%   0.0%
          6.1%   0.0%   0.0%   0.0%     Avg.:  6.1%
          6.6%   3.9%   0.0%   0.0%     Avg.:  5.3%
          5.5%   9.7%   6.0%   0.0%     Avg.:  7.1%
************************************************************************************************************
TAg Forg
          0.0%   0.0%   0.0%   0.0%
         16.2%   0.0%   0.0%   0.0%     Avg.: 16.2%
         19.9%  13.5%   0.0%   0.0%     Avg.: 16.7%
         22.0%  20.6%  16.2%   0.0%     Avg.: 19.6%
************************************************************************************************************
[Elapsed time = 0.5 h]
Done!